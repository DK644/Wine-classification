# -*- coding: utf-8 -*-
"""Классификация по качеству вина, используя RFC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DJN42S0rnGmx5JaqOBwSuyiAuITxF4h5

# Импорт библиотек
"""

! pip install pingouin

# Импорт библиотек и метрик

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Метрики
from sklearn.metrics import accuracy_score  as acc
from sklearn.metrics import f1_score as f1
from sklearn.metrics import precision_score as pre
from sklearn.metrics import recall_score as re

"""# Визуализация и предварительная обработка

**Анализ столбцов**

* fixed acidity (фиксированная кислотность) - обучловлена наличием нелетучих кислот (кислот, которые испаряются медленно)

* volatile acidity (летучая кислотность) - обусловлена наличием летучих кислот, отвечают за аромат вина, в тоже время слишком большое их количество снижает его качество

* citric acid (лимонная кислота) - практически отсутствует в красных винах, там она разрушается с образованием уксусной кислоты

* residual sugar (остаточный сахар) - природный сахар, оставшийся в вине, который дрожжам во время брожения не удалось преобразовать в спирт. Если уровень остаточного сахара высок, вино становится сладким на вкус.

* chlorides (хлориды) количество соли в вине

* free sulfur dioxide (свободный диоксид серы) - свободная форма SO2, предотвращает рост микроорганизмов и окисление вина

* total sulfur dioxide (общее количество SO2) - количество свободной и связанной форм SO2

* density (плотность) - плотность вина

* pH - кислотность, описывает насколько кислым является вино по шкале от 0 до 14

* sulphates (сульфаты) - добавка, которая способствует повышению уровня диоксида серы

* alcohol (алкоголь) - характеризует крепость вина

Вывод дата-фрейма

df - исходный датасет

df_cl - датасет после удаления выбросов
"""

# Чтение и вывод датафрейма
df = pd.read_csv("wine.csv")
df

"""Рассмотрим базовые статистики датасета

Мы можем сравнить среднее и медианное значение для каждого столбца, в большинстве случаев выборочное среднее и медианные значения довольно близки, значит эти распределения симметричные, но встречаются и распределения с довольно сильно различающимися средним и медианой
"""

df.describe()

"""Проверим данные на соответствие нормальному распределению используя критерий Шапиро-Уилка

Делаем вывод, что данные распределены не нормально

Повторим проверку после удаления выбросов
"""

# H0 - данные распределены нормально
# H1 -данные распределены не нормально

from scipy.stats import shapiro

alpha = 0.05

for col in df.columns:
  p = shapiro(df[col])[1]
  if p < alpha:
    print(f"Столбец {col} распределён ненормально")
  else:
    print(f"Столбец {col} распределён нормально")

"""Зададим функцию, которая бы выделяла численные и категориальные данные"""

# разбиение данных на числовые и категориальные, хотя на этапе df.info() мы уже выяснили, что все столбцы числовые
# По хорошему надо бы целевой столбец сделать категориальным, целевой столбец - порядковая шкала
def sep(DF):
  cat_columns = []
  num_columns = []

  for column_name in DF.columns:
      if (DF[column_name].dtypes == object):
          cat_columns +=[column_name]
      else:
          num_columns +=[column_name]

  return [cat_columns, num_columns]

"""Строим гистограммы столбцов, чтобы проверить наличие выбросов"""

# Визуализация плотности распреджеления столбцов
n_of_fig = len(sep(df)[1])
if n_of_fig%2 == 0:
  height = (n_of_fig//3)
  width = 3
else:
  height = n_of_fig//2
  width = 3
fig, ax = plt.subplots(nrows=height, ncols=width, figsize=(18,17))

for idx, column_name in enumerate(sep(df)[1]):
  plt.subplot(height, width, idx+1)
  sns.histplot(data=df, x=column_name, bins = 20);

"""Удаление выбросов"""

df_cl = df.copy()
# чистим столбец fixed acidity
q_fixed_ac = df_cl[df_cl['fixed acidity'] > 14]
df_cl = df_cl.drop(q_fixed_ac.index)

# чистим столбец volatile acidity
df_cl = df_cl.drop(df_cl[df_cl['volatile acidity']> 1.2].index)

# Чистим столбец citric acid
df_cl = df_cl.drop(df_cl[df_cl['citric acid']> 0.8].index)

# Чистим столбец residual sugar
df_cl = df_cl.drop(df_cl[df_cl['residual sugar']> 8].index)

# Чистим столбец chlorides
df_cl = df_cl.drop(df_cl[df_cl['chlorides']> 0.15].index)

# Чистим столбец free sulfur dioxide
df_cl = df_cl.drop(df_cl[df_cl['free sulfur dioxide']> 55].index)

# Чистим столбец total sulfur dioxide
df_cl = df_cl.drop(df_cl[df_cl['total sulfur dioxide']> 165].index)

# Чистим столбец pH
df_cl = df_cl.drop(df_cl[df_cl['pH']> 3.8].index)

# Чистим столбец sulphates
df_cl = df_cl.drop(df_cl[df_cl['sulphates']> 1.5].index)

# Чистим столбец alcohol
df_cl = df_cl.drop(df_cl[df_cl['alcohol']> 13.5].index)

# Обновляем индексы
df_cl = df_cl.reset_index(drop = True)

"""Повторно построим гистограммы столбцов после удаления выбросов"""

# n_of_fig = len(sep(df_cl)[1])
# if n_of_fig%2 == 0:
#   height = (n_of_fig//3)
#   width = 3
# else:
#   height = n_of_fig//2
#   width = 3
# fig, ax = plt.subplots(nrows=height, ncols=width, figsize=(18,17))

# for idx, column_name in enumerate(sep(df_cl)[1]):
#   plt.subplot(height, width, idx+1)
#   sns.histplot(data=df_cl, x=column_name, bins = 20);

"""После удаления выбросов повторно проведём тест на нормальность

Удаление выбросов не повлияло на распределение данных
"""

# H0 - данные распределены нормально
# H1 -данные распределены не нормально

from scipy.stats import shapiro

alpha = 0.05

for col in df_cl.columns:
  p = shapiro(df_cl[col])[1]
  if p < alpha:
    print(f"Столбец {col} распределён ненормально")
  else:
    print(f"Столбец {col} распределён нормально")

"""# Этап визуализации и анализа

Анализируя матрицу корреляции можно сказать, что на качество вина обратнопропорционально влияет количество летучих кислот и прямопропорционально - количество лимонной кислоты, сульфатов и содержание алкоголя

В датасете довольно много созависимых столбцов, то есть данные одного столбца явно зависят от данных другого, это плохо влияет на качество обучения модели, поэтому оставим только максимально независимые столбцы исходя из здравого смысла

* Можно удалить столбец pH, т.к. он явно зависит от столбцов fixed acidity, volatile acidity и citric acid, а также имеет небольшой коэффициент корреляции по отношению к целевому столбцу

* Под вопросом столбцы fixed acidity, volatile acidity, citric acid, эти столбцы нужно проверять на ложную корреляцию

* Можно удалить столбец density, т.к. плотность это функция многих параметров
"""

corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(8, 8))
sns.heatmap(corr_matrix, annot=True)
plt.show()
# Анализируя матрицу корреляции можно сказать, что на качество вина влияет количество летучих кислот (чем меньше, тем лучше), лимонной кислоты, количество сульфатов и содержание алкоголя

"""# Составляем пайплайн модели случайного леса

Составим пайплайн

Мы испытаем четыре варианта датасета на одном алгоритме:

1. Полный датасет, без удаления столбцов
2. Датасет с удалёнными зависимыми столбцами, согласно логике
3. Датасет с механически отобранными оптимальными признаками, используя функцию EFS
4. Датасет концентрированной информации, использовав метод главных компонент

В качестве метрик будем использовать f1-меру с макроусреднением, так как классы несбалансированы. Эта метрика должна показать наиболее объективный результат

**Вывод:** Проверив все 4 варианта, лучше всего себя показало использование полного датасета, наименьший результат был получен с использованием метода главных компонент

## Пайплайн Случайного леса, использовав все столбцы датасета
"""

# Импорт методов для пайплайна
# Импортируем класс pipeline
from sklearn.pipeline import Pipeline
# Импортируем класс для стандартизации
from sklearn.preprocessing import StandardScaler
# Импорт инструментов для создания модели МО
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score as f1

from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

model = RandomForestClassifier(n_estimators= 35)

# Разделили данные на тренировочные и тестовые
X_1 = df_cl[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
             'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
             'pH', 'sulphates', 'alcohol']]

Y_1 = df_cl["quality"]

from sklearn.model_selection import train_test_split #  функция разбиения на тренировочную и тестовую выборку
X_train, X_test, Y_train, Y_test = train_test_split(X_1, Y_1, test_size=0.3, random_state=42)

# Задаём элементы пайплайна
std_scaler = StandardScaler()
# Формируем пайплайн числовых данных
pipe_num = Pipeline([('scaler', std_scaler)])
# Задаём итоговый пайплайн
final_pipe = Pipeline([('preproc', pipe_num), ('model', model)])

# Обучаем пайплайн
final_pipe.fit(X_train, Y_train)
# Делаем предсказание на тестовых признаках
preds = final_pipe.predict(X_test)
# Считаем метрику
f1(Y_test, preds, average='macro')

"""## Пайплайн случайного леса, с удалёнными созависимыми столбцами

Для начала проверим столбцы fixed acidity, volatile acidity, citric acid на ложную корреляцию
"""

# Импортируем коэффициент коррелции Пирсона
from scipy.stats import pearsonr
# Импортируем функцию для нахождения частного коэффициента корреляции
import pingouin as pg

# H0 - величины некоррелированы

# Посчитаем обычный коэффициент корреляции Пирсона
# Столбец fixed acidity
corr, p_value = pearsonr(df_cl['fixed acidity'], df_cl['quality'])
print(f'fixed acidity: r = {corr}, p-Value = {p_value}')

# Столбец volatile acidity
corr, p_value = pearsonr(df_cl['volatile acidity'], df_cl['quality'])
print(f'volatile acidity: r = {corr}, p-Value = {p_value}')

# Столбец citric acid
corr, p_value = pearsonr(df_cl['citric acid'], df_cl['quality'])
print(f'citric acid: r = {corr}, p-Value = {p_value}')

pg.partial_corr(data = df_cl, x = 'fixed acidity', y = 'quality', covar = 'volatile acidity')

pg.partial_corr(data = df_cl, x = 'fixed acidity', y = 'quality', covar = 'citric acid')

pg.partial_corr(data = df_cl, x = 'volatile acidity', y = 'quality', covar = 'fixed acidity')

pg.partial_corr(data = df_cl, x = 'volatile acidity', y = 'quality', covar = 'citric acid')

pg.partial_corr(data = df_cl, x = 'citric acid', y = 'quality', covar = 'fixed acidity')

pg.partial_corr(data = df_cl, x = 'citric acid', y = 'quality', covar = 'volatile acidity')

"""таким образом, используя частный коэффициент корреляции делаем выводы:
* Столбец fixed acidity был зависим от столбцов citric acid и volatile acidity, этот столбец можно удалить
* Столбец citric acid зависим от столбца volatile acidity, также удаляем этот столбец

Наименьшему влиянию оказался подвергнут столбец volatile acidity, оставляем его, так как он имеет достаточно большую корреляцию с целевым признаком
"""

# Разделили данные на тренировочные и тестовые
X_2 = df_cl[['volatile acidity', 'residual sugar',
             'chlorides', 'free sulfur dioxide', 'total sulfur dioxide',
             'sulphates', 'alcohol']]

Y_2 = df_cl["quality"]

from sklearn.model_selection import train_test_split #  функция разбиения на тренировочную и тестовую выборку
X_train, X_test, Y_train, Y_test = train_test_split(X_2, Y_2, test_size=0.3, random_state=42)

# Задаём элементы пайплайна
std_scaler = StandardScaler()
# Формируем пайплайн числовых данных
pipe_num = Pipeline([('scaler', std_scaler)])
# Задаём итоговый пайплайн
final_pipe = Pipeline([('preproc', pipe_num), ('model', model)])

# Обучаем пайплайн
final_pipe.fit(X_train, Y_train)
# Делаем предсказание на тестовых признаках
preds = final_pipe.predict(X_test)
# Считаем метрику
f1(Y_test, preds, average='macro')

"""## Используем исчерпывающий отбор признаков для автоматического определения наиболее значимых столбцов"""

X_3_1 = df_cl[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
             'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
             'pH', 'sulphates', 'alcohol']]

Y_3_1 = df_cl["quality"]

from sklearn.model_selection import train_test_split #  функция разбиения на тренировочную и тестовую выборку
X_train, X_test, Y_train, Y_test = train_test_split(X_3_1, Y_3_1, test_size=0.3, random_state=42)

# Проведём отбор признаков, чтобы выявить наиболее значимые

# efs1 = EFS(model,
#            min_features=2,
#            max_features=5,
#            scoring='f1_macro',
#            print_progress=True,
#            cv = 4)

# efs1 = efs1.fit(X_train, Y_train)

# print('Best f1 score: %.2f' % efs1.best_score_)
# print('Best subset (indices):', efs1.best_idx_)
# print('Best subset (corresponding names):', efs1.best_feature_names_)

# Наиболее значимыми признаками считаются: 'volatile acidity', 'citric acid', 'free sulfur dioxide', 'sulphates', 'alcohol'

X_3_2 = df_cl[['volatile acidity', 'citric acid', 'free sulfur dioxide', 'sulphates', 'alcohol']]

Y_3_2 = df_cl["quality"]

from sklearn.model_selection import train_test_split #  функция разбиения на тренировочную и тестовую выборку
X_train, X_test, Y_train, Y_test = train_test_split(X_3_2, Y_3_2, test_size=0.3, random_state=42)

# Задаём элементы пайплайна
std_scaler = StandardScaler()
# Формируем пайплайн числовых данных
pipe_num = Pipeline([('scaler', std_scaler)])
# Задаём итоговый пайплайн
final_pipe = Pipeline([('preproc', pipe_num), ('model', model)])

# Обучаем пайплайн
final_pipe.fit(X_train, Y_train)
# Делаем предсказание на тестовых признаках
preds = final_pipe.predict(X_test)
# Считаем метрику
f1(Y_test, preds, average='macro')

"""## Используем PCA для составления концентрированного датасета"""

# Импортируем класс для использования метода главных компонент
from sklearn.decomposition import PCA
# Импорт библиотеки для работы с массивами
import numpy as np

"""Изначально нам нужно определить, какого количества главных компонент будет достаточно для качественного обучения модели

Поэтому сначала мы используем все столбцы и получим максимальное количество главных компонент, затем посмотрим какой вклад вносит каждая из них
"""

X_4 = df_cl[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',
             'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates',
             'alcohol']]

Y_4 = df_cl["quality"].astype('category')

X_train, X_test, Y_train, Y_test = train_test_split(X_4, Y_4, test_size=0.3, random_state=42)

# Используем максимальное количество гланвых компонент
pca = PCA(n_components=11)

# Обучаем меторд главных компонент на всех доступных данных
pca.fit(X_train)

# Построим график кумулятивной объяснённой дисперсии
# Судя по графику, четырёх главных компонент будет достаточно
sns.set(style = 'whitegrid')
plt.plot(np.cumsum(pca.explained_variance_ratio_))

plt.xlabel('Число главных компонент')
plt.ylabel('Кумулятивная объяснённая дисперсия')

plt.axvline(linewidth=4, color = 'r', linestyle = '--', x = 4, ymin = 0, ymax = 1)
display(plt.show())

"""Для большей наглядности выведем таблицу Объяснённой дисперсии и кумулятивной объяснённой дисперсии

Видим, что уже на чётвёртой главной компонентекумулятивная дисперсия крайне близка к единице, а на девятом шаге единица достигается
"""

evr = pca.explained_variance_ratio_
cvr = np.cumsum(pca.explained_variance_ratio_)

pca_df = pd.DataFrame()
pca_df['Кумулятивная объяснённая дисперсия'] = cvr
pca_df['Объяснённая дисперсия'] = evr
display(pca_df.head(11))

"""Согласно результатам первичного анализа четырёх компонент будет достаточно, поэтому теперь заново используем PCA, но уже задав 4 главных компоненты, а не 11"""

pca = PCA(n_components=4)

pca.fit(X_train)

X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

# Задаём элементы пайплайна
std_scaler = StandardScaler()
# Формируем пайплайн числовых данных
pipe_num = Pipeline([('scaler', std_scaler)])
# Задаём итоговый пайплайн
final_pipe = Pipeline([('preproc', pipe_num), ('model', model)])

# Обучаем пайплайн
final_pipe.fit(X_train_pca, Y_train)
# Делаем предсказание на тестовых признаках
preds = final_pipe.predict(X_test_pca)
# Считаем метрику
f1(Y_test, preds, average='macro')

"""# Анализ данных"""

sns.pairplot(df_cl, height= 2, hue = 'quality', palette = 'tab10')

"""Что если разделить исходный датасет на несколько более сбалансированных датасетов?"""

df_low_target = df_cl.copy()

df_low_target = df_low_target.drop(df_low_target[df_low_target['quality'] == 5].index)
df_low_target = df_low_target.drop(df_low_target[df_low_target['quality'] == 6].index)
df_low_target = df_low_target.drop(df_low_target[df_low_target['quality'] == 7].index)

# Обновляем индексы
df_low_target = df_low_target.reset_index(drop = True)

sns.pairplot(data = df_low_target, height= 2, hue = 'quality', palette = 'tab10')

# Разделили данные на тренировочные и тестовые
X = df_low_target[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
             'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
             'pH', 'sulphates', 'alcohol']]

Y = df_low_target["quality"]

from sklearn.model_selection import train_test_split #  функция разбиения на тренировочную и тестовую выборку
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Задаём элементы пайплайна
std_scaler = StandardScaler()
# Формируем пайплайн числовых данных
pipe_num = Pipeline([('scaler', std_scaler)])
# Задаём итоговый пайплайн
final_pipe = Pipeline([('preproc', pipe_num), ('model', model)])

# Обучаем пайплайн
final_pipe.fit(X_train, Y_train)
# Делаем предсказание на тестовых признаках
preds = final_pipe.predict(X_test)
# Считаем метрику
f1(Y_test, preds, average='macro')

df_high_target = df_cl.copy()

df_high_target = df_high_target.drop(df_high_target[df_high_target['quality'] == 3].index)
df_high_target = df_high_target.drop(df_high_target[df_high_target['quality'] == 4].index)
df_high_target = df_high_target.drop(df_high_target[df_high_target['quality'] == 8].index)

# Обновляем индексы
df_high_target = df_high_target.reset_index(drop = True)

sns.pairplot(data = df_high_target, height= 2, hue = 'quality', palette = 'tab10')

# Разделили данные на тренировочные и тестовые
X = df_high_target[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
             'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
             'pH', 'sulphates', 'alcohol']]

Y = df_high_target["quality"]

from sklearn.model_selection import train_test_split #  функция разбиения на тренировочную и тестовую выборку
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Задаём элементы пайплайна
std_scaler = StandardScaler()
# Формируем пайплайн числовых данных
pipe_num = Pipeline([('scaler', std_scaler)])
# Задаём итоговый пайплайн
final_pipe = Pipeline([('preproc', pipe_num), ('model', model)])

# Обучаем пайплайн
final_pipe.fit(X_train, Y_train)
# Делаем предсказание на тестовых признаках
preds = final_pipe.predict(X_test)
# Считаем метрику
f1(Y_test, preds, average='macro')

"""Таким образом мы лишний раз убедились, что модели машинного обучения работают лучше при более менее сбалансированных классах

Чем ещё можно дополнить этот проект?

1. Проверить статистическую значимость корреляции
2. Многие действия над данными стоит перевести в функции
3. Добавить кросс-валидацию
4. Использовать другие модели МО, выяснить, какая сработает лучше на сбалансированном частичном датасете
5. Добавить авто-подбор наилучших гиперпараметров
6. Более подробно интерпретировать метрики и сделать вывод
"""

